# ğŸ“„ deploy_kegs/train/finetune.py - Whisper ëª¨ë¸ì„ 4ë¹„íŠ¸ ì–‘ìí™”ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

import os
import glob
import sys
import json
import torch
from pathlib import Path
from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorWithPadding, BitsAndBytesConfig
from datasets import load_dataset, concatenate_datasets, Audio
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# GPU ê°€ìš©ì„± í™•ì¸
print(torch.cuda.is_available())

# ì„¤ì • ë¡œë“œ
with open("train/config.json", "r", encoding="utf-8") as f:
    config = json.load(f)

MODEL_NAME = config["base_model"]
OUTPUT_DIR = config["output_dir"]
DATASET_PATH = config["dataset_path"]
SAMPLE_RATE = 16000

def load_model_and_processor():
    # 1) 4ë¹„íŠ¸ ì–‘ìí™” ë¡œë“œ ì„¤ì •
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        llm_int8_threshold=6.0,
        llm_int8_has_fp16_weight=False
    )
    # 2) ëª¨ë¸ ë¡œë“œ
    model = WhisperForConditionalGeneration.from_pretrained(
        MODEL_NAME,
        quantization_config=quant_config,
        device_map="auto",
        local_files_only=False
    )
    # 3) k-bit ì–‘ìí™” í•™ìŠµ ì¤€ë¹„
    model = prepare_model_for_kbit_training(model)

    # 4) Processor ë¡œë“œ
    processor = WhisperProcessor.from_pretrained(
        MODEL_NAME,
        language="ko",
        task="transcribe",
        local_files_only=False
    )
    # 5) LoRA ì–´ëŒ‘í„° ì„¤ì •
    peft_config = LoraConfig(
        r=8,
        target_modules=["q_proj", "v_proj"],
        lora_alpha=16,
        lora_dropout=0.05,
        bias="none"
    )
    model = get_peft_model(model, peft_config)
    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language="ko", task="transcribe")
    model.eval()
    return model, processor

# def load_model_and_processor():
#     processor = WhisperProcessor.from_pretrained(MODEL_NAME)
#     model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)
#     model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language="ko", task="transcribe")
#     return model, processor

def load_combined_dataset():
    tsv_files = [str(p) for p in Path(DATASET_PATH).glob("*.tsv")]
    if not tsv_files:
        print("[ê²½ê³ ] í•™ìŠµìš© .tsv ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¹ˆ ëª¨ë¸ë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
        return None

    datasets = []
    for tsv_file in tsv_files:
        ds = load_dataset("csv", data_files={"train": tsv_file}, delimiter="\t")["train"]
        datasets.append(ds)

    combined = concatenate_datasets(datasets)
    combined = combined.map(lambda x: {**x, "audio": x["audio"].replace("\\", "/")}, desc="ê²½ë¡œ ìŠ¬ë˜ì‹œ ì •ê·œí™”")
    combined = combined.cast_column("audio", Audio(sampling_rate=16000))
    return combined

def prepare_dataset(batch, processor):
    audio = batch["audio"]
    batch["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=SAMPLE_RATE,
        return_tensors="pt"
    ).input_features[0]
    labels = processor.tokenizer(
        batch["text"], padding=True, truncation=True
        ).input_ids
    batch["labels"] = labels
    return batch

class DataCollatorWhisper:
    def __init__(self, processor):
        self.processor = processor

    def __call__(self, features):
        input_features = [f["input_features"] for f in features]
        label_features = [f["labels"] for f in features]

        batch = {
            "input_features": torch.tensor(input_features, dtype=torch.float32),
            "labels": torch.tensor(self.processor.tokenizer.pad(
                {"input_ids": label_features}, padding=True, return_tensors="pt"
            )["input_ids"], dtype=torch.long)
        }
        return batch

def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    model, processor = load_model_and_processor()

    dataset = load_combined_dataset()
    if dataset is None:
        model.save_pretrained(OUTPUT_DIR)
        processor.save_pretrained(OUTPUT_DIR)
        print("[ì™„ë£Œ] ë¹ˆ ëª¨ë¸ë§Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    dataset = dataset.map(
        lambda x: prepare_dataset(x, processor),
        remove_columns=dataset.column_names,
        batched=False
    )

    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=2,
        num_train_epochs=3,
        fp16=torch.cuda.is_available(),
        learning_rate=1e-4,
        logging_dir="./logs",
        save_steps=200,
        save_total_limit=2,
        optim="paged_adamw_32bit",
		gradient_checkpointing=True,
        report_to=None
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=processor,
        data_collator=DataCollatorWhisper(processor),
    )

    trainer.train()
    model.save_pretrained(OUTPUT_DIR)
    processor.save_pretrained(OUTPUT_DIR)
    print("[ì™„ë£Œ] ëª¨ë¸ íŒŒì¸íŠœë‹ ë° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
