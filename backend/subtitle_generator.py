# ğŸ“„ backend/subtitle_generator.py - Whisperë¡œë¶€í„° SRT ìë§‰ ìƒì„±

import os
import re
import torch
import torchaudio
from moviepy import VideoFileClip
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from konlpy.tag import Komoran
import uuid
import config

vad_model, utils = torch.hub.load('snakers4/silero-vad', 'silero_vad', force_reload=False)
get_speech_timestamps = utils[0]

SAMPLE_RATE = 16000
MIN_VALID_TEXT_LEN = 5

def calculate_char_positions(sentence, morphs):
    """ë¬¸ì¥ì—ì„œ í˜•íƒœì†Œ ë‹¨ì–´ë“¤ì´ ì‹œì‘í•˜ëŠ” ìœ„ì¹˜ ê³„ì‚°"""
    positions = []
    p = 0
    for morph in morphs:
        while p < len(sentence) and sentence[p] == ' ':
            p += 1
        start = p
        for c in morph:
            if p < len(sentence) and sentence[p] == c:
                p += 1
            else:
                break
        positions.append(start)
    return positions

def find_morph_end_position(sentence, start_pos, morph):
    """Whisper ì›ë¬¸ì—ì„œ morphì˜ ëë‚˜ëŠ” ì‹¤ì œ ìœ„ì¹˜ë¥¼ ì°¾ìŒ"""
    p = start_pos
    m = 0  # morphì˜ ì¸ë±ìŠ¤

    while p < len(sentence) and m < len(morph):
        if sentence[p] == morph[m]:
            m += 1
        p += 1

    return p  # ë ìœ„ì¹˜ (exclusive)

def improved_semantic_split(text, video_id, min_len=12, max_len=80):
    """
    ê¸´ Whisper ìë§‰ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³ , ë””ë²„ê·¸ ë¡œê·¸ ì €ì¥.

    Args:
        text (str): Whisperë¡œë¶€í„° ìƒì„±ëœ í…ìŠ¤íŠ¸.
        video_id (str): ë””ë²„ê·¸ ë¡œê·¸ íŒŒì¼ëª…ì— ì‚¬ìš©í•  ID.
        min_len (int): í•˜ë‚˜ì˜ ìë§‰ ìµœì†Œ ê¸¸ì´ (ê¸°ë³¸ 12ì).
        max_len (int): í•˜ë‚˜ì˜ ìë§‰ ìµœëŒ€ ê¸¸ì´ (ê¸°ë³¸ 80ì).
    
    Returns:
        list: ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ìë§‰ ë¦¬ìŠ¤íŠ¸.
    """
    os.makedirs("../logs", exist_ok=True)
    log_path = f"../logs/split_debug_{video_id}.txt"
    log_file = open(log_path, "a", encoding="utf-8")

    komoran = Komoran()
    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    subtitle_chunks = []

    log_file.write(f"âœ… [0] ì›ë³¸ í…ìŠ¤íŠ¸:\n{text}\n\n")
    log_file.write(f"âœ… [1] ë¬¸ì¥ ë¶€í˜¸ ê¸°ì¤€ 1ì°¨ ë¶„í•  ê²°ê³¼:\n")
    for s in sentences:
        log_file.write(f"- {s}\n")
    log_file.write("\n")

    for sentence in sentences:
        tokens = komoran.pos(sentence)
        morphs = [morph for morph, _ in tokens]
        morph_positions = calculate_char_positions(sentence, morphs)

        split_indices = set()
        ec_candidates = []

        for idx, (word, pos) in enumerate(tokens):
            if pos == 'EF':
                end_pos = morph_positions[idx] + len(word)
                if end_pos < len(sentence) and sentence[end_pos] == ' ':
                    split_indices.add(end_pos)
            elif pos == 'MAJ' or (word == ',' and pos == 'SP'):
                split_indices.add(morph_positions[idx] + len(word))

        for ec_idx, (morph, pos) in enumerate(tokens):
            if pos == 'EC':
                has_vv = any(
                    ec_idx - hop >= 0 and tokens[ec_idx - hop][1].startswith('VV')
                    for hop in range(1, 4)
                )
                if has_vv:
                    ec_start_pos = morph_positions[ec_idx]
                    ec_end_pos = find_morph_end_position(sentence, ec_start_pos, morph)

                    ec_candidates.append((ec_idx, ec_end_pos))

                    # EC ëë‚œ ë°”ë¡œ ë‹¤ìŒì´ ê³µë°±ì´ë©´ ë¶„ë¦¬
                    if ec_end_pos < len(sentence) and sentence[ec_end_pos] == ' ':
                        split_indices.add(ec_end_pos)

        split_indices = sorted(split_indices)

        log_file.write(f"ğŸ¯ ë¬¸ì¥: {sentence}\n")
        log_file.write(f"âœ… [2] í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼:\n{tokens}\n\n")
        log_file.write(f"âœ… [3] EC í›„ë³´ ì¸ë±ìŠ¤:\n")
        for ec_idx, ec_end_pos in ec_candidates:
            log_file.write(f"- EC index: {ec_idx}, EC end pos: {ec_end_pos}\n")
        log_file.write("\n")
        log_file.write(f"âœ… [4] split_indices:\n{split_indices}\n\n")

        chunks = []
        last = 0
        for idx in split_indices:
            chunk = sentence[last:idx].strip()
            if chunk:
                chunks.append(chunk)
            last = idx
        if last < len(sentence):
            chunk = sentence[last:].strip()
            if chunk:
                chunks.append(chunk)

        log_file.write(f"âœ… [5] ì´ˆë²Œ ë¶„í•  ê²°ê³¼:\n")
        for c in chunks:
            log_file.write(f"- {c}\n")
        log_file.write("\n")

        merged_chunks = []
        if len(chunks) == 1:
            merged_chunks.append(chunks[0])
        else:
            i = 0
            while i < len(chunks):
                if len(chunks[i]) >= min_len:
                    merged_chunks.append(chunks[i])
                    i += 1
                else:
                    if i == 0:
                        merged_chunk = chunks[i] + " " + chunks[i + 1]
                        merged_chunks.append(merged_chunk.strip())
                        i += 2
                    elif i == len(chunks) - 1:
                        merged_chunks[-1] += " " + chunks[i]
                        i += 1
                    else:
                        merged_chunk = chunks[i] + " " + chunks[i + 1]
                        merged_chunks.append(merged_chunk.strip())
                        i += 2

        log_file.write(f"âœ… [6] ì§§ì€ ì ˆ ë³‘í•© ê²°ê³¼:\n")
        for m in merged_chunks:
            log_file.write(f"- {m}\n")
        log_file.write("\n")

        subtitle_chunks.extend(merged_chunks)

    final_subtitles = []
    for chunk in subtitle_chunks:
        if len(chunk) > max_len:
            tokens = komoran.pos(chunk)
            morphs = [word for word, _ in tokens]

            split_points = []
            for idx, (word, pos) in enumerate(tokens):
                if pos == 'SP' and word == ',':
                    split_points.append(idx)
                if pos == 'MAJ':
                    split_points.append(idx)

            char_positions = []
            char_idx = 0
            for idx, word in enumerate(morphs):
                char_positions.append(char_idx)
                char_idx += len(word)
                if idx < len(morphs) - 1:
                    char_idx += 1

            candidate_chars = [char_positions[i] for i in split_points if i < len(char_positions)]

            split_point = None
            if candidate_chars:
                mid = len(chunk) // 2
                split_point = min(candidate_chars, key=lambda x: abs(x - mid))
            else:
                spaces = [m.start() for m in re.finditer(r'\s', chunk)]
                if spaces:
                    split_point = min(spaces, key=lambda x: abs(x - len(chunk) // 2))

            if split_point:
                first = chunk[:split_point + 1].strip()
                second = chunk[split_point + 1:].strip()
                final_subtitles.append(first)
                final_subtitles.append(second)
            else:
                final_subtitles.append(chunk.strip())
        else:
            final_subtitles.append(chunk.strip())

    log_file.write(f"âœ… [7] ìµœì¢… ê²°ê³¼:\n")
    for m in final_subtitles:
        log_file.write(f"- {m}\n")
    log_file.write("\n====================\n\n")
    log_file.close()
    print(f"[ğŸªµ ë””ë²„ê·¸] ë¶„ë¦¬ ê²°ê³¼ ë¡œê·¸ ì €ì¥ ì™„ë£Œ â†’ '{log_path}'")

    return final_subtitles

def format_srt_time(seconds):
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    ms = int((seconds - int(seconds)) * 1000)
    return f"{h:02}:{m:02}:{s:02},{ms:03}"

def generate_srt_from_video(video_path, srt_output_path, model_path=config.MODEL_PATH):
    print("[ğŸ¬] ì˜ìƒì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘...")
    audio_path = video_path.replace(".mp4", ".wav")

    clip = VideoFileClip(video_path)
    clip.audio.write_audiofile(audio_path, fps=SAMPLE_RATE, nbytes=2, codec="pcm_s16le", ffmpeg_params=["-ac", "1"])
    clip.close()

    print("[ğŸ§ ] Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")
    processor = WhisperProcessor.from_pretrained(model_path, local_files_only=True)
    model = WhisperForConditionalGeneration.from_pretrained(model_path, local_files_only=True)
    model.generation_config.forced_decoder_ids = processor.get_decoder_prompt_ids(language="ko", task="transcribe")
    model.eval()

    waveform, sr = torchaudio.load(audio_path)
    if sr != SAMPLE_RATE:
        waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)
    waveform = waveform.squeeze(0)

    print("[ğŸ”] ìŒì„± êµ¬ê°„ ê°ì§€ ì¤‘...")
    speech_timestamps = get_speech_timestamps(waveform, vad_model, sampling_rate=SAMPLE_RATE)
    print(f"[âœ…] ê°ì§€ëœ ìŒì„± êµ¬ê°„ ìˆ˜: {len(speech_timestamps)}")

    segments = []
    segment_index = 1
    video_id = str(uuid.uuid4())[:8]

    # raw_transcript.txt ë®ì–´ì“°ê¸°
    os.makedirs("../logs", exist_ok=True)
    with open("../logs/raw_transcript.txt", "w", encoding="utf-8") as tf:
        tf.write("")  # ì´ˆê¸°í™”

    for ts in speech_timestamps:
        start_sec = ts['start'] / SAMPLE_RATE
        end_sec = ts['end'] / SAMPLE_RATE
        chunk = waveform[ts['start']:ts['end']]
        duration = end_sec - start_sec

        input_features = processor.feature_extractor(
            chunk.numpy(), sampling_rate=SAMPLE_RATE, return_tensors="pt"
        ).input_features

        # decoder_input_ids = torch.tensor([processor.get_decoder_prompt_ids(language="ko", task="transcribe")], device=model.device)
        with torch.no_grad():
            predicted_ids = model.generate(
                input_features=input_features,
                # decoder_input_ids=decoder_input_ids,
                max_length=448,
                suppress_tokens=[]
            )

        decoded = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0].strip()
        if not decoded or len(decoded) < MIN_VALID_TEXT_LEN:
            continue

        # Whisper ì›ë¬¸ ì €ì¥
        with open("../logs/raw_transcript.txt", "a", encoding="utf-8") as tf:
            tf.write(decoded + "\n\n")

        # Improved split
        lines = improved_semantic_split(decoded, video_id, min_len=12, max_len=80)

        total_chars = sum(len(line) for line in lines)
        local_time = start_sec

        for line in lines:
            proportion = len(line) / total_chars
            line_duration = proportion * duration
            segments.append({
                "index": segment_index,
                "start": local_time,
                "end": local_time + line_duration,
                "text": line
            })
            segment_index += 1
            local_time += line_duration

    print(f"[âœ…] ìµœì¢… ìë§‰ ì¤„ ìˆ˜: {len(segments)}")

    with open(srt_output_path, "w", encoding="utf-8-sig") as f:
        for seg in segments:
            f.write(f"{seg['index']}\n")
            f.write(f"{format_srt_time(seg['start'])} --> {format_srt_time(seg['end'])}\n")
            f.write(f"{seg['text']}\n\n")